# Gradient Descent on our Linear Regression

How do we solve the partial derivative term?
How do we take the derivative of a summation?
There is an answer for this

Linear regression will always have a convex function or a Bow Shaped function Has only one global optima, since there is only one minima

Batch Gradient Descent. Each step of the gradient descent uses all the training examples.

The summation alwasy hits all M values in the training set


There is a way to solve for the minimum instead of using an iterative method like gradient descent. It is called the normal equation method
